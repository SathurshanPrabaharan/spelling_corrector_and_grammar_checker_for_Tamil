{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG0t9hW5SoeO",
        "outputId": "79c64174-02ad-4eac-fabe-bb947654e1bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import sentencepiece as spm\n",
        "import re\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "file_path = \"/content/drive/MyDrive/AI/Error Annotated Corpus.csv\"  # Update path as needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df_cleaned = df[['Error word & consecutive word', 'Corrected words & its', 'Annotation']].dropna()\n",
        "df_cleaned.columns = ['error_text', 'corrected_text', 'annotation']\n",
        "\n",
        "# Clean Tamil text\n",
        "def clean_tamil_text(text):\n",
        "    text = re.sub(r'[^\\u0B80-\\u0BFF\\s]', '', str(text))  # Adjusted Unicode range for Tamil\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df_cleaned['error_text'] = df_cleaned['error_text'].apply(clean_tamil_text)\n",
        "df_cleaned['corrected_text'] = df_cleaned['corrected_text'].apply(clean_tamil_text)\n",
        "\n",
        "# Combine all text for tokenizer training\n",
        "all_text = df_cleaned['error_text'].tolist() + df_cleaned['corrected_text'].tolist()\n",
        "with open(\"tamil_text.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(all_text))\n",
        "\n",
        "# Train a SentencePiece tokenizer\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=\"tamil_text.txt\", model_prefix=\"tamil\", vocab_size=5000, model_type=\"unigram\"\n",
        ")\n",
        "tokenizer = spm.SentencePieceProcessor(model_file=\"tamil.model\")\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "max_length = 50\n",
        "def tokenize_and_pad(texts):\n",
        "    sequences = [tokenizer.encode_as_ids(text) for text in texts]\n",
        "    return pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Grammar Correction Task\n",
        "gc_X = tokenize_and_pad(df_cleaned['error_text'])\n",
        "gc_y = tokenize_and_pad(df_cleaned['corrected_text'])\n",
        "# Remove the squeeze operation:\n",
        "# gc_y = gc_y.squeeze(-1)  # Adjust shape for sparse categorical cross-entropy\n",
        "\n",
        "gc_X_train, gc_X_test, gc_y_train, gc_y_test = train_test_split(gc_X, gc_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Classification Task\n",
        "label_encoder = LabelEncoder()\n",
        "df_cleaned['annotation'] = label_encoder.fit_transform(df_cleaned['annotation'])\n",
        "cls_X = tokenize_and_pad(df_cleaned['error_text'])\n",
        "cls_y = df_cleaned['annotation']\n",
        "\n",
        "cls_X_train, cls_X_test, cls_y_train, cls_y_test = train_test_split(cls_X, cls_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Grammar Correction Model (Seq2Seq)\n",
        "gc_vocab_size = tokenizer.vocab_size()\n",
        "gc_input = Input(shape=(max_length,))\n",
        "gc_embedding = Embedding(input_dim=gc_vocab_size, output_dim=256)(gc_input)\n",
        "gc_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "gc_lstm_out, _, _ = gc_lstm(gc_embedding)\n",
        "gc_dense = Dense(gc_vocab_size, activation=\"softmax\")(gc_lstm_out)\n",
        "gc_model = Model(gc_input, gc_dense)\n",
        "gc_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Early stopping and learning rate reduction\n",
        "gc_early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "gc_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "\n",
        "print(\"Training Grammar Correction Model...\")\n",
        "gc_history = gc_model.fit(\n",
        "    gc_X_train, gc_y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    callbacks=[gc_early_stop, gc_reduce_lr]\n",
        ")\n",
        "\n",
        "# Error Classification Model\n",
        "cls_vocab_size = tokenizer.vocab_size()\n",
        "cls_input = Input(shape=(max_length,))\n",
        "cls_embedding = Embedding(input_dim=cls_vocab_size, output_dim=256)(cls_input)\n",
        "cls_lstm = Bidirectional(LSTM(128))(cls_embedding)\n",
        "cls_dropout = Dropout(0.5)(cls_lstm)\n",
        "cls_output = Dense(len(label_encoder.classes_), activation=\"softmax\")(cls_dropout)\n",
        "cls_model = Model(cls_input, cls_output)\n",
        "cls_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "cls_early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "cls_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "\n",
        "print(\"Training Classification Model...\")\n",
        "cls_history = cls_model.fit(\n",
        "    cls_X_train, cls_y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    callbacks=[cls_early_stop, cls_reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate Models\n",
        "print(\"Evaluating Grammar Correction Model...\")\n",
        "gc_y_pred = np.argmax(gc_model.predict(gc_X_test, verbose=0), axis=-1)\n",
        "gc_eval = gc_model.evaluate(gc_X_test, gc_y_test, verbose=0)\n",
        "print(\"Grammar Correction Model - Loss: {:.4f}, Accuracy: {:.4f}\".format(gc_eval[0], gc_eval[1]))\n",
        "\n",
        "print(\"Evaluating Classification Model...\")\n",
        "cls_eval = cls_model.evaluate(cls_X_test, cls_y_test, verbose=0)\n",
        "print(\"Classification Model - Loss: {:.4f}, Accuracy: {:.4f}\".format(cls_eval[0], cls_eval[1]))\n",
        "\n",
        "# Save Models\n",
        "gc_model.save(\"grammar_correction_model.h5\")\n",
        "cls_model.save(\"error_classification_model.h5\")\n",
        "\n",
        "# Utility to decode sequences\n",
        "def decode_sequence(sequence):\n",
        "    return tokenizer.decode_ids(sequence.tolist())\n",
        "\n",
        "# Load pre-trained grammar correction model\n",
        "gc_model = load_model(\"grammar_correction_model.h5\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpaMPIbLcKZ-",
        "outputId": "9a9643de-e10d-4634-a0b3-97e88b08a5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Grammar Correction Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 843ms/step - accuracy: 0.8811 - loss: 2.8329 - val_accuracy: 0.9297 - val_loss: 0.6304 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 865ms/step - accuracy: 0.9298 - loss: 0.6089 - val_accuracy: 0.9299 - val_loss: 0.6131 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 818ms/step - accuracy: 0.9303 - loss: 0.5853 - val_accuracy: 0.9301 - val_loss: 0.6001 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 845ms/step - accuracy: 0.9305 - loss: 0.5616 - val_accuracy: 0.9301 - val_loss: 0.5795 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 824ms/step - accuracy: 0.9317 - loss: 0.5291 - val_accuracy: 0.9306 - val_loss: 0.5647 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 826ms/step - accuracy: 0.9317 - loss: 0.5039 - val_accuracy: 0.9304 - val_loss: 0.5583 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 864ms/step - accuracy: 0.9325 - loss: 0.4859 - val_accuracy: 0.9304 - val_loss: 0.5569 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 832ms/step - accuracy: 0.9328 - loss: 0.4716 - val_accuracy: 0.9305 - val_loss: 0.5564 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 861ms/step - accuracy: 0.9333 - loss: 0.4594 - val_accuracy: 0.9311 - val_loss: 0.5549 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 867ms/step - accuracy: 0.9347 - loss: 0.4429 - val_accuracy: 0.9315 - val_loss: 0.5547 - learning_rate: 0.0010\n",
            "Training Classification Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 221ms/step - accuracy: 0.2735 - loss: 2.4752 - val_accuracy: 0.3893 - val_loss: 2.0659 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 210ms/step - accuracy: 0.4015 - loss: 2.0250 - val_accuracy: 0.4577 - val_loss: 1.7788 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 225ms/step - accuracy: 0.5997 - loss: 1.4290 - val_accuracy: 0.5510 - val_loss: 1.6395 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 208ms/step - accuracy: 0.8304 - loss: 0.7088 - val_accuracy: 0.5622 - val_loss: 1.6176 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 207ms/step - accuracy: 0.9161 - loss: 0.3608 - val_accuracy: 0.5697 - val_loss: 1.7522 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 208ms/step - accuracy: 0.9580 - loss: 0.1964 - val_accuracy: 0.5510 - val_loss: 1.9307 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 209ms/step - accuracy: 0.9766 - loss: 0.1245 - val_accuracy: 0.5808 - val_loss: 1.8924 - learning_rate: 5.0000e-04\n",
            "Evaluating Grammar Correction Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 37 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7df202bf7010> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammar Correction Model - Loss: 0.5567, Accuracy: 0.9316\n",
            "Evaluating Classification Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Model - Loss: 1.7751, Accuracy: 0.5299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get user input and predict corrected sentence\n",
        "while True:\n",
        "    user_input = input(\"Enter a Tamil sentence with grammatical errors (or 'exit' to quit): \").strip()\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    cleaned_input = clean_tamil_text(user_input)\n",
        "    tokenized_input = pad_sequences([tokenizer.encode_as_ids(cleaned_input)], maxlen=max_length, padding='post')\n",
        "    predicted_output = np.argmax(gc_model.predict(tokenized_input, verbose=0), axis=-1)\n",
        "    corrected_sentence = decode_sequence(predicted_output[0])\n",
        "    print(\"Corrected Sentence:\", corrected_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp3u3ujDnL_p",
        "outputId": "5d50a51c-0826-46be-83d4-73f7820b19ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a Tamil sentence with grammatical errors (or 'exit' to quit): பேசாம ஏன்\n",
            "Corrected Sentence:  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
            "Enter a Tamil sentence with grammatical errors (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import sentencepiece as spm\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load models and tokenizer\n",
        "gc_model = tf.keras.models.load_model(\"grammar_correction_model.h5\")\n",
        "tokenizer = spm.SentencePieceProcessor(model_file=\"tamil.model\")\n",
        "\n",
        "# Function to decode sequences\n",
        "def decode_sequence(encoded_sequence):\n",
        "    \"\"\"Convert sequence of token IDs back to a sentence.\"\"\"\n",
        "    decoded_words = tokenizer.decode_ids([id_ for id_ in encoded_sequence if id_ > 0])\n",
        "    return decoded_words\n",
        "\n",
        "# Set tf.function outside loop for optimized prediction\n",
        "@tf.function\n",
        "def predict_corrected_sentence(padded_input):\n",
        "    \"\"\"Predict corrected sequence for input.\"\"\"\n",
        "    predicted_output = gc_model(padded_input, training=False)\n",
        "    predicted_sequence = tf.argmax(predicted_output, axis=-1)[0]  # Removed .numpy() call\n",
        "    return predicted_sequence\n",
        "\n",
        "# Main input loop\n",
        "max_length = 50\n",
        "while True:\n",
        "    user_input = input(\"Enter a Tamil sentence with grammatical errors (or 'exit' to quit): \").strip()\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    # Tokenize and pad input sequence\n",
        "    tokenized_input = tokenizer.encode_as_ids(user_input)\n",
        "    padded_input = pad_sequences([tokenized_input], maxlen=max_length, padding='post')\n",
        "\n",
        "    # Predict and decode corrected output\n",
        "    corrected_sequence = predict_corrected_sentence(tf.constant(padded_input, dtype=tf.int32))\n",
        "    corrected_sentence = decode_sequence(corrected_sequence)\n",
        "\n",
        "    print(\"Corrected Sentence:\", corrected_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzVSlS5MkdAb",
        "outputId": "95c56a0d-1ed6-4013-998a-c55edb378761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a Tamil sentence with grammatical errors (or 'exit' to quit): பேசாம ஏன்\n",
            "Corrected Sentence: \n",
            "Enter a Tamil sentence with grammatical errors (or 'exit' to quit): exit\n"
          ]
        }
      ]
    }
  ]
}